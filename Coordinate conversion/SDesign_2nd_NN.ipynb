{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import csv\n",
    "import numpy as np\n",
    "import SD2_Model as SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 1, 5]              15\n",
      "            Linear-2                 [-1, 1, 5]              30\n",
      "            Linear-3                 [-1, 1, 5]              30\n",
      "            Linear-4                 [-1, 1, 5]              30\n",
      "            Linear-5                 [-1, 1, 2]              12\n",
      "================================================================\n",
      "Total params: 117\n",
      "Trainable params: 117\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#net = SD.Net()\n",
    "device = torch.device(\"cuda\") \n",
    "net = SD.Net().to(device)\n",
    "summary(net, input_size = (1,2))\n",
    "#print(net)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#dummy data\n",
    "#trainset = (torch.rand(2,1, device=\"cuda\"), torch.rand(2,1, device=\"cuda\") ) #Xs and Ys\n",
    "trainset = torch.tensor([[1,2],[3,4]]).to(device)\n",
    "print(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([253., 151.], device='cuda:0') tensor([ 0.4324, -2.2259], device='cuda:0')\n",
      "tensor([-0.4666,  0.3340], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Load_data = True\n",
    "dataset_path = \"../2NN_dataset_flight.3.csv\"\n",
    "if(Load_data):\n",
    "    trainset = []\n",
    "    with open(dataset_path) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            #feature, data_label = int(row[0:2]),row[2:4]\n",
    "            feature, data_label = (int(row[0]),int(row[1])),(float(row[2]) * 100 ,float(row[3]) * 100) #convert to cm\n",
    "            #feature, data_label = (int(row[0]),int(row[1])),(round(float(row[2]) * 100) ,round(float(row[3]) * 100)) #convert to cm\n",
    "            trainset.append([feature, data_label])\n",
    "            #print([feature, data_label])\n",
    "\n",
    "trainset = torch.Tensor(trainset).to(device)\n",
    "#print(trainset)\n",
    "\n",
    "test_data = True\n",
    "if(test_data):\n",
    "    #trainset = np.array(trainset)\n",
    "    #print(trainset[1])\n",
    "    feature, data_label = trainset[0]\n",
    "    print(feature, data_label)\n",
    "    #print(feature.view(1,2))\n",
    "    output = net(feature)\n",
    "    print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(518.8115, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "1 tensor(140.6212, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "2 tensor(30.9913, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "3 tensor(2.0027, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "4 tensor(12.5325, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "5 tensor(13.0900, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "6 tensor(13.0536, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "7 tensor(12.5798, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "8 tensor(9.3403, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "9 tensor(8.5636, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "10 tensor(7.2635, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "11 tensor(9.3594, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "12 tensor(11.0619, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "13 tensor(10.4241, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "14 tensor(9.2326, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "15 tensor(7.4865, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "16 tensor(24.9018, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "17 tensor(108.5868, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "18 tensor(105.1477, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "19 tensor(99.8643, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "20 tensor(94.2711, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "21 tensor(94.0434, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "22 tensor(92.9024, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "23 tensor(75.0595, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "24 tensor(50.0727, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "25 tensor(46.1098, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "26 tensor(37.1787, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "27 tensor(29.3686, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "28 tensor(30.0895, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "29 tensor(29.4565, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "30 tensor(30.3058, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "31 tensor(30.9654, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "32 tensor(20.0539, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "33 tensor(17.9141, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "34 tensor(17.8881, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "35 tensor(14.9235, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "36 tensor(14.2654, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "37 tensor(11.6302, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "38 tensor(10.7766, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "39 tensor(11.4519, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "40 tensor(13.2325, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "41 tensor(14.9783, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "42 tensor(16.3499, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "43 tensor(13.2188, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "44 tensor(12.7669, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "45 tensor(13.0778, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "46 tensor(13.1214, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "47 tensor(12.4109, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "48 tensor(11.0738, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "49 tensor(10.0164, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "50 tensor(9.9178, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "51 tensor(9.9662, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "52 tensor(7.4821, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "53 tensor(8.4673, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "54 tensor(6.0466, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "55 tensor(5.6110, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "56 tensor(5.0690, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "57 tensor(4.7886, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "58 tensor(4.5979, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "59 tensor(4.1931, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "60 tensor(3.6923, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "61 tensor(3.4423, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "62 tensor(3.6564, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "63 tensor(3.5360, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "64 tensor(3.5410, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "65 tensor(3.3475, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "66 tensor(3.3443, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "67 tensor(3.3865, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "68 tensor(3.1653, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "69 tensor(2.9807, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "70 tensor(2.8760, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "71 tensor(3.0942, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "72 tensor(2.6439, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "73 tensor(2.3445, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "74 tensor(1.7766, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "75 tensor(1.5596, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "76 tensor(1.5474, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "77 tensor(1.5349, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "78 tensor(1.0308, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "79 tensor(0.5786, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "80 tensor(0.7930, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "81 tensor(0.9188, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "82 tensor(0.8460, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "83 tensor(0.8934, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "84 tensor(0.7679, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "85 tensor(0.5454, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "86 tensor(0.4290, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "87 tensor(0.2074, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "88 tensor(0.3458, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "89 tensor(0.3622, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "90 tensor(0.3275, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "91 tensor(0.2463, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "92 tensor(0.2827, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "93 tensor(0.2248, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "94 tensor(0.2048, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "95 tensor(0.2010, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "96 tensor(0.1558, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "97 tensor(0.1509, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "98 tensor(0.1484, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "99 tensor(0.1153, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "0.11530748009681702\n"
     ]
    }
   ],
   "source": [
    "train = True\n",
    "epochs = 100\n",
    "best_loss = 0.0\n",
    "if (train):\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        for data in trainset:  \n",
    "            feature, data_label = data #\n",
    "            #print(data)\n",
    "            #print(feature)\n",
    "            #print(data_label)\n",
    "            net.zero_grad()  \n",
    "            output = net(feature)\n",
    "            loss = loss_function(output, data_label) \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "        print(epoch,loss)\n",
    "        if(epoch == 1):\n",
    "            best_loss = loss.item()\n",
    "        if(loss.item() < best_loss or loss.item() < 1.0):\n",
    "            best_loss = loss.item()\n",
    "            best_save = \"./models_saves/best_2nd_NN_\"\n",
    "            torch.save(net, best_save + str(epoch) + \"_\" + str(round(best_loss,5)) +\".pt\")\n",
    "    print(best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save network\n",
    "save = True\n",
    "#save = False\n",
    "model_save_path = \"2nd_NN.pt\"\n",
    "if (save):\n",
    "    torch.save(net, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test evaluate\n",
    "dataset_path = \"../2NN_dataset_flight.2.csv\"\n",
    "if(False):\n",
    "    net.eval()\n",
    "    eval_data = []\n",
    "    with open(dataset_path) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            #feature, data_label = int(row[0:2]),row[2:4]\n",
    "            feature, data_label = (int(row[0]),int(row[1])),(float(row[2]) * 100 ,float(row[3]) * 100) #convert to cm\n",
    "            #feature, data_label = (int(row[0]),int(row[1])),(round(float(row[2]) * 100) ,round(float(row[3]) * 100)) #convert to cm\n",
    "            net(feature)\n",
    "            eval_data.append([feature, data_label])\n",
    "            \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58dc188af903f85495c8c8ba2d08d4f5901f4034aea6d9dbb2f8641074b88212"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('191T_py3': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
